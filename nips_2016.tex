\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
% \PassOptionsToPackage{numbers, compress}{natbib}
% before loading nips_2016
%
% to avoid loading the natbib package, add option nonatbib:
% \usepackage[nonatbib]{nips_2016}

\usepackage[final]{nips_2016}

% to compile a camera-ready version, add the [final] option, e.g.:
% \usepackage[final]{nips_2016}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{amsmath}

\title{Voxel-wise nonlinear analysis toolbox for neurodegenerative diseases and aging}

% The \author macro works with any number of authors. There are two
% commands used to separate the names and addresses of multiple
% authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to
% break the lines. Using \AND forces a line break at that point. So,
% if LaTeX puts 3 of 4 authors names on the first line, and the last
% on the second line, try using \AND instead of \And before the third
% author name.

\author{
  Authors\\
  Signal Theory and Communications Department\\
  Universitat Politècnica de Catalunya, BarcelonaTech\\
  Barcelona, Spain \\
  \texttt{email}\\
  %% \AND
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
}

\begin{document}
% \nipsfinalcopy is no longer used

\maketitle

\begin{abstract}
  The abstract paragraph should be indented \nicefrac{1}{2}~inch
  (3~picas) on both the left- and right-hand margins. Use 10~point
  type, with a vertical spacing (leading) of 11~points.  The word
  \textbf{Abstract} must be centered, bold, and in point size 12. Two
  line spaces precede the abstract. The abstract must be limited to
  one paragraph.
\end{abstract}

\section{Introduction}

\section{The toolbox}
\label{toolbox}

The toolbox comprises an independent \textit{fitting library}, made up of different \textit{model fitting} and \textit{fit evaluation} methods, a \textit{processing} module that interacts with the aforementioned \textit{fitting library} providing the formatted data obtained from the \textit{file system}, several \textit{visualization} tools and a \textit{CLI interface} that allows the interaction between the user and the \textit{processing} module, supported by a \textit{configuration file}. 

\subsection{Model fitting techniques}

A model fitting consists on finding a parametric or a nonparametric function of some explanatory variables (\textbf{predictors}) and possibly some confound variables (\textbf{correctors}) that best fits the observations of the target variable in terms of a given quality metric or, conversely, that minimizes the loss between the prediction of the model and the actual observations.
\begin{itemize}
\item \textbf{General Linear Model (GLM)} 

The General Linear model is a generalization of multiple linear regression to the case of more than one dependent variable. As in the case of multiple linear regression, the most common lost function is the Residual Sum of Squares, and the optimization procedure used is Ordinary Least Squares, which yields the well-known normal equation $ X^TX\beta = X^Ty $, and from that we solve for the $\beta$ parameters to obtain the final solution: $ \beta = (X^TX)^{-1}X^Ty $.

A possible approach to model nonliniearities with this model is a polynomial basis expansion of degree $d$, that is, the input space $\mathfrak{X}$ is mapped into another feature space $\mathfrak{F}$ that also includes the polynomial terms of the variables: ($\Phi : \mathfrak{X} \rightarrow \mathfrak{F}$).

\item \textbf{Generalized Additive Model (GAM)} 

A Generalized Additive Model is a Generalized Linear Model in which the observations of the target variable depend linearly on unknown smooth functions of some predictor variables: $ f(X) = \alpha + \sum_{i=1}^{k} f_i(X_i)$. Here $f_1, f_2, ..., f_k$ are nonparametric smooth functions that are simultaneously estimated using scatterplot smoothers by means of the \textbf{backfitting algorithm}. Several fitting methods can be accomodated in this framework by using different smoother operators, such as cubic splines, polynomial or Gaussian smoothers. 

\item \textbf{Support Vector Regression (SVR)} 

The regression counterpart of the well-known Support Vector Machines, Support Vector Regression, is based on the following idea: the goal is to find a function that has at most $\epsilon$ deviation from the observations and, at the same time, is as flat as possible. However, the $\epsilon$ deviation contraint is not feasible sometimes, and a hyperparameter that controls the degree up to which deviations larger than $\epsilon$ are tolerated is introduced, $C$. The linear function for SVR is $ f(x) = \langle w , x \rangle + b $, and then the solution for the optimization problem is $ f(x) = \sum_{i=1}^{n} (\alpha_i - \alpha^*_i)\langle x_i , x \rangle + b $, where $ (\alpha_i - \alpha^*_i) $ are the coefficients found in the dual optimization problem.

In context of SVR the nonlinearities are introduced with the "kernel trick", that is, a kernel function $ k(x_i, x_j) = \langle \Phi(x_i), \Phi(x_j) \rangle $ is introduced that implicitly maps the inputs from their original space into another high-dimensional space without requiring to know the explicit mapping $ \Phi(\cdot) $. The solution using the kernel function is then $ f(x) = \sum_{i=1}^{n} (\alpha_i - \alpha^*_i)k(x_i, x) + b$. 

The kernel function used in this toolbox is the Radial Basis Function or Gaussian kernel, which is defined as $ k(x_i, x_j) = exp(-\gamma \|x_i - x_j\|^2)$.
\end{itemize}

\subsection{Hyperparameters search algorithm}

\subsubsection{Error functions}
\begin{enumerate}
	\item MSE
	\item Cp statistic
	\item ANOVA-based error
\end{enumerate}

\subsection{Fit evaluation methods}
\begin{itemize}
\item \textbf{F-test} 

The F-test can be used in regression problems to determine whether a particular part of a model is significantly improving the overall performance of the rest of the model.
The F-test compares the variance of the error of the restricted model $M_{restricted}$, which consists only in the correctors, with the variance of the error of the full model $M_{full}$, which consists both in the correctors and the predictors, and evaluates whether the variance of the full model is significantly lower — from a statistical point of view — than the variance of the restricted model, that is, the inclusion of the predictors contributes to the explanation of the observations.

The F-statistic is defined as:
\begin{equation}
F = \frac{(\frac{RSS_{restricted} - RSS_{full}}{df_{restricted} - df_{full}})}{(\frac{RSS_{full}}{df_{full}})}
\end{equation}

Under the null hypothesis F will follow a F-distribution of parameters $(df_{restricted} - df_{full}, df_{full})$.

One can notice that this statistical test requires then the degrees of freedom of both models, which are trivial to compute in GLM, but they aren't in GAM or SVR. The equivalent degrees of freedom for Support Vector Regression were introduced in the toolbox as defined in \ref{}.

\item \textbf{PRSS\footnote{Penalized Residual Sum of Squares}, Variance-Normalized PRSS} 

Penalized Residuals Sum of Squares is introduced in the toolbox in order to provide a fit evaluation metric that penalizes the complexity of the predicted curve without requiring the degrees of freedom. However, PRSS is not suitable enough in the context of morphometric analysis, as it always provides better scores for target variables with low-variance and flat trends than for target variables with high-variance and nonflat trends, and that poses a problem as most of the voxels in the brain are 0-valued. 

For that reason a variance normalized version of the PRSS that takes into account this domain specific requirement and weights the score with the inverse of the variance of the predicted curve was introduced, the Variance Normalized Penalized Residuals Sum of Squares, which is formulated as $\frac{1}{\frac{1}{n-1} \sum_{i=1}^{n} (\hat{y}_i - \frac{1}{n} \sum_{i=1}^{n} \hat{y}_i)^2} (\sum_{i=1}^{n} (y_i - \hat{y}_i)^2 + \lambda \int{}^{} [f^{''}(x)]^2 dx)$.

\end{itemize}

\subsection{Interactive visualization tools}

\section{Implementation details}

The whole toolbox has been implemented in Python. Numpy and scipy have been used for the numerical and scientific computing, NiBabel for handling the morphometric data in NIfTI format, scikit-learn for the machine learning algorithms and matplotlib and seaborn for plotting and for the visualization features.

\section{Experiments}

\subsection{Dataset}

\section{Conclusions}

\subsubsection*{Acknowledgments}


\section*{References}
\small

[1] Alexander, J.A.\ \& Mozer, M.C.\ (1995) Template-based algorithms
for connectionist rule extraction. In G.\ Tesauro, D.S.\ Touretzky and
T.K.\ Leen (eds.), {\it Advances in Neural Information Processing
  Systems 7}, pp.\ 609--616. Cambridge, MA: MIT Press.

\end{document}
